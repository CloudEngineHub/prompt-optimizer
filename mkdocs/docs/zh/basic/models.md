# 模型管理

本指南介绍如何配置和管理 Prompt Optimizer 支持的各种AI模型。

## 🤖 支持的AI模型

### 主流模型提供商
- **OpenAI** - GPT-4, GPT-3.5等系列模型
- **Gemini** - Google的Gemini Pro系列
- **DeepSeek** - DeepSeek Chat和Coder系列
- **智谱AI** - GLM-4系列模型
- **SiliconFlow** - 多种开源模型集成
- **自定义模型** - 支持任意OpenAI兼容接口

## ⚙️ 模型配置（符合正常流程测试）

### 配置步骤
1. **打开模型管理**
   - 点击右上角的“⚙️ 模型管理”
   - 弹出“模型配置”对话框（包含 OpenAI/Gemini/…）

2. **添加 API 密钥**
   - 选择要配置的模型提供商
   - 输入对应的 API 密钥
   - 点击“测试连接”（应显示清晰的成功/失败提示）

3. **保存配置**
   - 点击“保存”完成配置
   - 再次打开应保持原配置（持久化）
   - 主界面模型选择器应包含新配置的模型（状态指示正常）

### 高级参数配置

除了API密钥，还可以为每个模型设置高级参数：

**常用参数**：
- **Temperature** - 控制输出的随机性 (0.0-1.0)
- **Max Tokens** - 限制输出的最大长度
- **Top P** - 控制采样的多样性
- **Frequency Penalty** - 降低重复内容的概率

**配置示例**：
```json
{
  "temperature": 0.7,
  "max_tokens": 4096,
  "top_p": 0.9,
  "frequency_penalty": 0.1
}
```

## 🎛️ 模型选择与状态

### 界面选择
- 主界面右上角可切换当前使用的模型
- 状态指示器显示可用/异常（与“测试连接”一致）

### 选择策略

### 根据用途选择
- **日常使用** - GPT-4或Gemini Pro，平衡性能与成本
- **创意写作** - 使用较高的Temperature (0.7-0.9)
- **技术文档** - 使用较低的Temperature (0.1-0.3)
- **本地部署** - 配置Ollama等本地模型

### 成本优化
1. **成本考虑** - 日常使用可选择成本较低的模型
2. **质量要求** - 重要任务使用更强大的模型
3. **速度需求** - 需要快速响应时选择速度较快的模型
4. **隐私要求** - 敏感内容可使用本地部署的模型

## 🔧 故障排除

### 连接问题
1. **API密钥错误**
   - 检查密钥是否正确复制
   - 确认密钥是否有效期内
   - 验证API权限设置

2. **网络问题**
   - 检查网络连接状态
   - 确认防火墙设置
   - 尝试使用代理服务器

3. **设置不生效/丢失**
   - 关闭后重新打开模型管理核对配置是否持久
   - 刷新页面后检查模型选择器是否仍可用
   - 清理浏览器缓存前先导出数据备份

3. **额度不足**
   - 检查API账户余额
   - 确认使用限制设置
   - 考虑升级账户套餐

### 性能优化
- **合理配置参数** - 避免不必要的token消耗
- **缓存常用配置** - 保存经常使用的模型设置
- **监控使用情况** - 定期检查API使用统计

---

**相关链接**：
- [连接问题](../help/connection-issues.md) - 解决连接和网络问题
- [数据管理](data.md) - 导出/导入全局配置与上下文集合
